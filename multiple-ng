# Locally run a test - create two nodegroups - one per AZ - create a deployment per AZ  using nodeSelector and ensure that scaling works for both AZ

# Create Multiple nodegroups - hardcode the subnet IDs


# To avoid the duplicate creation of the SG rules for multiple nodegroups
# In deploy/main.tf
# In nodegroup block
# For first nodegroup:
create_nodegroup_ssh_and_nodeports_rule = true
# For additional nodegroups:
create_nodegroup_ssh_and_nodeports_rule = false

# In modules/aws-eks-managed-nodegroup directory

# In locals.tf
create_nodegroup_ssh_and_nodeports_rule = false

# In secgroups.tf
# Set this count in the AllowSSH and the NodeGroup SG rule
count = var.ssh_access_eks_workers && local.managed_node_group["create_nodegroup_ssh_and_nodeports_rule"] ? 1 : 0


# How to deploy applications to different AZs with EBS and Cluster Autoscaler

## Every node will have the label of the nodegroup
topology.kubernetes.io/zone=us-east-1b

# These labels (e.g. topology.kubernetes.io/zone=us-west-2b)  will automatically be added to your nodes via the Kubernetes cloud provider and automatically added to your PersistentVolumes via the AWS EBS CSI driver. This means that the initial pod placement and EBS volume provisioning is transparent to you when the pod is initially scheduled. This label can be used to deploy EBS dependent workloads
# Read blog: https://aws.amazon.com/blogs/containers/amazon-eks-cluster-multi-zone-auto-scaling-groups/

#If a pod using an EBS volume is rescheduled or the instance is terminated, you will need to make sure your pod has a nodeSelector that matches your topology label
